# CUDA_VISIBLE_DEVICES=0  NPROC_PER_NODE=1  MAX_PIXELS=602112  swift rlhf  --rlhf_type grpo  --train_type lora  --load_from_cache_file true  --torch_dtype bfloat16  --learning_rate 1e-6  --max_steps 100  --warmup_ratio 0.05  --logging_steps 5  --eval_steps 500  --save_steps 500  --save_total_limit 2  --output_dir output  --dataloader_num_workers 1  --attn_impl flash_attn  --use_vllm true  --vllm_mode colocate  --vllm_gpu_memory_utilization 0.55  --vllm_tensor_parallel_size 1  --vllm_max_model_len 2048  --max_length 2048  --max_completion_length 1024  --num_generations 4  --per_device_train_batch_size 1  --per_device_eval_batch_size 1  --gradient_accumulation_steps 4  --temperature 1.0  --top_p 1.0  --top_k 80  --deepspeed zero3_offload  --offload_optimizer true  --offload_model true  --log_completions true  --system examples/train/grpo/prompt.txt  --reward_funcs smart_accuracy format  --dataset AI-ModelScope/chartqa_digit_r1v_format  --model Qwen/Qwen3-VL-4B-Instruct
# CUDA_VISIBLE_DEVICES=0  NPROC_PER_NODE=1  MAX_PIXELS=602112  swift rlhf  --rlhf_type grpo  --train_type lora  --load_from_cache_file true  --torch_dtype bfloat16  --learning_rate 1e-6  --max_steps 100  --warmup_ratio 0.05  --logging_steps 5  --eval_steps 500  --save_steps 500  --save_total_limit 2  --output_dir output  --dataloader_num_workers 1  --attn_impl flash_attn  --use_vllm true  --vllm_mode colocate  --vllm_gpu_memory_utilization 0.55  --vllm_tensor_parallel_size 1  --vllm_max_model_len 2048  --max_length 2048  --max_completion_length 1024  --num_generations 4  --per_device_train_batch_size 1  --per_device_eval_batch_size 1  --gradient_accumulation_steps 4  --temperature 1.0  --top_p 1.0  --top_k 80  --deepspeed zero3_offload  --offload_optimizer true  --offload_model true  --log_completions true  --system examples/train/grpo/prompt.txt  --reward_funcs smart_accuracy format  --dataset AI-ModelScope/chartqa_digit_r1v_format  --model Qwen/Qwen3-VL-2B-Thinking
# CUDA_VISIBLE_DEVICES=0  NPROC_PER_NODE=1  MAX_PIXELS=602112  swift rlhf  --rlhf_type grpo  --train_type lora  --load_from_cache_file true  --torch_dtype bfloat16  --learning_rate 1e-6  --max_steps 100  --warmup_ratio 0.05  --logging_steps 5  --eval_steps 500  --save_steps 500  --save_total_limit 2  --output_dir output  --dataloader_num_workers 1  --attn_impl flash_attn  --use_vllm true  --vllm_mode colocate  --vllm_gpu_memory_utilization 0.55  --vllm_tensor_parallel_size 1  --vllm_max_model_len 2048  --max_length 2048  --max_completion_length 1024  --num_generations 4  --per_device_train_batch_size 1  --per_device_eval_batch_size 1  --gradient_accumulation_steps 4  --temperature 1.0  --top_p 1.0  --top_k 80  --deepspeed zero0  --offload_optimizer true  --offload_model true  --log_completions true  --system examples/train/grpo/prompt.txt  --reward_funcs smart_accuracy format  --dataset AI-ModelScope/chartqa_digit_r1v_format  --model BytedanceDouyinContent/SAIL-VL2-2B-Thinking
# CUDA_VISIBLE_DEVICES=0  NPROC_PER_NODE=1  MAX_PIXELS=602112  swift rlhf  --rlhf_type grpo  --train_type lora  --load_from_cache_file true  --torch_dtype bfloat16  --learning_rate 1e-6  --max_steps 100  --warmup_ratio 0.05  --logging_steps 5  --eval_steps 500  --save_steps 500  --save_total_limit 2  --output_dir output  --dataloader_num_workers 1  --attn_impl flash_attn  --use_vllm true  --vllm_mode colocate  --vllm_gpu_memory_utilization 0.55  --vllm_tensor_parallel_size 1  --vllm_max_model_len 2048  --max_length 2048  --max_completion_length 1024  --num_generations 4  --per_device_train_batch_size 1  --per_device_eval_batch_size 1  --gradient_accumulation_steps 4  --temperature 1.0  --top_p 1.0  --top_k 80  --deepspeed zero0  --offload_optimizer true  --offload_model true  --log_completions true  --system examples/train/grpo/prompt.txt  --reward_funcs smart_accuracy format  --dataset AI-ModelScope/chartqa_digit_r1v_format  --model LLM-Research/gemma-3-4b-it
# CUDA_VISIBLE_DEVICES=0  NPROC_PER_NODE=1  MAX_PIXELS=602112  swift rlhf  --rlhf_type grpo  --train_type lora  --load_from_cache_file true  --torch_dtype bfloat16  --learning_rate 1e-6  --max_steps 100  --warmup_ratio 0.05  --logging_steps 5  --eval_steps 500  --save_steps 500  --save_total_limit 2  --output_dir output  --dataloader_num_workers 1  --attn_impl flash_attn  --use_vllm true  --vllm_mode colocate  --vllm_gpu_memory_utilization 0.50  --vllm_tensor_parallel_size 1  --vllm_max_model_len 2048  --max_length 2048  --max_completion_length 1024  --num_generations 4  --per_device_train_batch_size 1  --per_device_eval_batch_size 1  --gradient_accumulation_steps 4  --temperature 1.0  --top_p 1.0  --top_k 80  --deepspeed zero0  --offload_optimizer true  --offload_model true  --log_completions true  --system examples/train/grpo/prompt.txt  --reward_funcs smart_accuracy format  --dataset AI-ModelScope/chartqa_digit_r1v_format  --model LLM-Research/Phi-3.5-vision-instruct
# CUDA_VISIBLE_DEVICES=0  NPROC_PER_NODE=1  MAX_PIXELS=602112  swift rlhf  --rlhf_type grpo  --train_type lora  --load_from_cache_file true  --torch_dtype bfloat16  --learning_rate 1e-6  --max_steps 100  --warmup_ratio 0.05  --logging_steps 5  --eval_steps 500  --save_steps 500  --save_total_limit 2  --output_dir output  --dataloader_num_workers 1  --attn_impl flash_attn  --use_vllm true  --vllm_mode colocate  --vllm_gpu_memory_utilization 0.55  --vllm_tensor_parallel_size 1  --vllm_max_model_len 2048  --max_length 2048  --max_completion_length 1024  --num_generations 4  --per_device_train_batch_size 1  --per_device_eval_batch_size 1  --gradient_accumulation_steps 4  --temperature 1.0  --top_p 1.0  --top_k 80  --deepspeed zero0  --offload_optimizer true  --offload_model true  --log_completions true  --system examples/train/grpo/prompt.txt  --reward_funcs accuracy format  --dataset ahmed-masry/ChartQAPro  --model LLM-Research/gemma-3-4b-it
# CUDA_VISIBLE_DEVICES=0  NPROC_PER_NODE=1  MAX_PIXELS=602112  swift rlhf  --rlhf_type grpo  --train_type lora  --load_from_cache_file true  --torch_dtype bfloat16  --learning_rate 1e-6  --max_steps 100  --warmup_ratio 0.05  --logging_steps 5  --eval_steps 500  --save_steps 500  --save_total_limit 2  --output_dir output  --dataloader_num_workers 1  --attn_impl flash_attn  --use_vllm true  --vllm_mode colocate  --vllm_gpu_memory_utilization 0.55  --vllm_tensor_parallel_size 1  --vllm_max_model_len 2048  --max_length 2048  --max_completion_length 1024  --num_generations 4  --per_device_train_batch_size 1  --per_device_eval_batch_size 1  --gradient_accumulation_steps 4  --temperature 1.0  --top_p 1.0  --top_k 80  --deepspeed zero0  --offload_optimizer true  --offload_model true  --log_completions true  --system examples/train/grpo/prompt.txt  --reward_funcs smart_accuracy format  --dataset /data/datasets/chart2code/json/chartrl.jsonl  --model Qwen/Qwen2.5-VL-3B-Instruct
# CUDA_VISIBLE_DEVICES=0 swift infer --val_dataset /data/datasets/slake/slake_test.jsonl --max_new_tokens 1024 --max_model_len 1024 --model Qwen/Qwen3-VL-32B-Thinking --quant_method bnb --quant_bits 4 --torch_dtype bfloat16 --stream true
# CUDA_VISIBLE_DEVICES=0 swift infer --val_dataset /data/datasets/slake/slake_test.jsonl#10 --max_new_tokens 1024 --max_model_len 1024 --quant_method bnb --quant_bits 4 --torch_dtype bfloat16 --stream true --model Qwen/Qwen3-VL-30B-A3B-Thinking
# CUDA_VISIBLE_DEVICES=0 swift infer --val_dataset /data/datasets/charxiv/charxiv_val.jsonl --max_new_tokens 1024 --max_model_len 1024 --quant_method bnb --quant_bits 4 --torch_dtype bfloat16 --stream true --model Qwen/Qwen3-VL-32B-Thinking
# CUDA_VISIBLE_DEVICES=0 swift infer --val_dataset /data/datasets/slake/slake_test.jsonl#10 --max_new_tokens 1024 --max_model_len 1024 --quant_method bnb --quant_bits 4 --torch_dtype bfloat16 --stream true --model Qwen/Qwen3-VL-32B-Instruct
# CUDA_VISIBLE_DEVICES=0 swift infer --val_dataset /data/datasets/charxiv/charxiv_val.jsonl --max_new_tokens 8192 --max_model_len 8192 --stream true --model Qwen/Qwen3-VL-4B-Instruct
# CUDA_VISIBLE_DEVICES=0 swift infer --val_dataset /data/datasets/vqarad/vqarad_test.jsonl --system "Answer the question concisely based on the image." --max_new_tokens 8192 --max_model_len 8192 --stream true --model Qwen/Qwen3-VL-2B-Instruct
# CUDA_VISIBLE_DEVICES=0 swift infer --val_dataset /data/datasets/vqarad/vqarad_test.jsonl --system "Answer the question concisely based on the image." --max_new_tokens 8192 --max_model_len 8192 --stream true --model Qwen/Qwen3-VL-4B-Instruct
# CUDA_VISIBLE_DEVICES=0 swift infer --val_dataset /data/datasets/vqarad/vqarad_test.jsonl --system "Answer the question concisely based on the image." --max_new_tokens 8192 --max_model_len 8192 --quant_method bnb --quant_bits 4 --torch_dtype bfloat16 --stream true --model Qwen/Qwen3-VL-32B-Instruct
# CUDA_VISIBLE_DEVICES=0  NPROC_PER_NODE=1  MAX_PIXELS=602112  swift rlhf  --rlhf_type grpo  --train_type lora  --load_from_cache_file true  --torch_dtype bfloat16  --learning_rate 1e-6  --max_steps 100  --warmup_ratio 0.05  --logging_steps 1  --eval_steps 500  --save_steps 500  --save_total_limit 2  --output_dir output  --dataloader_num_workers 1  --attn_impl flash_attn  --use_vllm true  --vllm_mode colocate  --vllm_gpu_memory_utilization 0.55  --vllm_tensor_parallel_size 1  --vllm_max_model_len 4096  --max_length 2048  --max_completion_length 2048  --num_generations 4  --per_device_train_batch_size 1  --per_device_eval_batch_size 1  --gradient_accumulation_steps 4  --temperature 1.0  --top_p 1.0  --top_k 80  --system examples/train/grpo/prompt.txt  --log_completions true  --offload_optimizer true  --offload_model true  --deepspeed zero3_offload  --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl  --reward_funcs smart_accuracy format  --model Qwen/Qwen3-VL-2B-Instruct
# CUDA_VISIBLE_DEVICES=0  NPROC_PER_NODE=1  MAX_PIXELS=602112  swift rlhf  --rlhf_type grpo  --train_type lora  --load_from_cache_file true  --torch_dtype bfloat16  --learning_rate 1e-6  --max_steps 100  --warmup_ratio 0.05  --logging_steps 1  --eval_steps 500  --save_steps 500  --save_total_limit 2  --output_dir output  --dataloader_num_workers 1  --attn_impl flash_attn  --use_vllm true  --vllm_mode colocate  --vllm_gpu_memory_utilization 0.55  --vllm_tensor_parallel_size 1  --vllm_max_model_len 4096  --max_length 2048  --max_completion_length 2048  --num_generations 4  --per_device_train_batch_size 1  --per_device_eval_batch_size 1  --gradient_accumulation_steps 4  --temperature 1.0  --top_p 1.0  --top_k 80  --system examples/train/grpo/prompt.txt  --log_completions true  --offload_optimizer true  --offload_model true  --deepspeed zero3_offload  --reward_funcs smart_accuracy format  --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl  --model Qwen/Qwen3-VL-2B-Instruct
# CUDA_VISIBLE_DEVICES=0  NPROC_PER_NODE=1  MAX_PIXELS=602112  swift rlhf  --rlhf_type grpo  --train_type lora  --load_from_cache_file true  --torch_dtype bfloat16  --learning_rate 1e-6  --max_steps 100  --warmup_ratio 0.05  --logging_steps 1  --eval_steps 500  --save_steps 500  --save_total_limit 2  --output_dir output  --dataloader_num_workers 1  --attn_impl flash_attn  --use_vllm true  --vllm_mode colocate  --vllm_gpu_memory_utilization 0.55  --vllm_tensor_parallel_size 1  --vllm_max_model_len 2048  --max_length 1024  --max_completion_length 1024  --num_generations 4  --per_device_train_batch_size 1  --per_device_eval_batch_size 1  --gradient_accumulation_steps 4  --temperature 1.0  --top_p 1.0  --top_k 80  --system examples/train/grpo/prompt.txt  --log_completions true  --offload_optimizer true  --offload_model true  --deepspeed zero0  --reward_funcs smart_accuracy format  --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl  --model LLM-Research/gemma-3-4b-it
# CUDA_VISIBLE_DEVICES=0  NPROC_PER_NODE=1  MAX_PIXELS=602112  swift rlhf  --rlhf_type grpo  --train_type lora  --load_from_cache_file true  --torch_dtype bfloat16  --learning_rate 1e-6  --max_steps 5000  --warmup_ratio 0.05  --logging_steps 1  --eval_steps 500  --save_steps 500  --save_total_limit 2  --output_dir output  --dataloader_num_workers 1  --attn_impl flash_attn  --use_vllm true  --vllm_mode colocate  --vllm_gpu_memory_utilization 0.55  --vllm_tensor_parallel_size 1  --vllm_max_model_len 2048  --max_length 1024  --max_completion_length 1024  --num_generations 4  --per_device_train_batch_size 1  --per_device_eval_batch_size 1  --gradient_accumulation_steps 4  --temperature 1.0  --top_p 1.0  --top_k 80  --system examples/train/grpo/prompt.txt  --log_completions true  --offload_optimizer true  --offload_model true  --deepspeed zero0  --reward_funcs smart_accuracy format  --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl  --model Qwen/Qwen2.5-VL-3B-Instruct
# CUDA_VISIBLE_DEVICES=0 swift infer --val_dataset /data/datasets/vqarad/vqarad_test.jsonl --system examples/train/grpo/prompt.txt --max_new_tokens 2048 --max_model_len 2048 --stream true --adapters /data/workspace/swift/output/v54-20251117-030137/checkpoint-500
# CUDA_VISIBLE_DEVICES=0 swift infer --val_dataset /data/datasets/vqarad/vqarad_test.jsonl --system "Answer the question concisely based on the image." --max_new_tokens 2048 --max_model_len 2048 --stream true --model Qwen/Qwen2.5-VL-3B-Instruct
# CUDA_VISIBLE_DEVICES=0  NPROC_PER_NODE=1  MAX_PIXELS=602112  swift rlhf  --rlhf_type grpo  --train_type lora  --load_from_cache_file true  --torch_dtype bfloat16  --learning_rate 5e-6  --max_steps 10000  --warmup_ratio 0.05  --logging_steps 10  --eval_steps 1000  --save_steps 1000  --save_total_limit 10  --output_dir output  --dataloader_num_workers 1  --attn_impl flash_attn  --use_vllm true  --vllm_mode colocate  --vllm_gpu_memory_utilization 0.55  --vllm_tensor_parallel_size 1  --vllm_max_model_len 2048  --max_length 2048  --max_completion_length 1024  --num_generations 4  --per_device_train_batch_size 1  --per_device_eval_batch_size 1  --gradient_accumulation_steps 4  --temperature 0.9  --top_p 0.9  --top_k 100  --system examples/train/grpo/prompt.txt  --log_completions true  --offload_optimizer true  --offload_model true  --deepspeed zero0  --reward_funcs answer_match_string plane_match_string modality_match_string caption_match_cosine format  --reward_weights 0.60 0.10 0.10 0.10 0.10  --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl  --model Qwen/Qwen2.5-VL-3B-Instruct  --report_to wandb  
# CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-5 --lr_scheduler_type cosine --max_steps 10000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 10 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.55 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 4 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --temperature 0.8 --top_p 0.9 --top_k 100 --system examples/train/grpo/prompt.txt --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs format answer_match_string plane_match_string modality_match_string caption_match_cosine --reward_weights 0.20 0.40 0.15 0.15 0.10 --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-5 --lr_scheduler_type cosine --max_steps 5000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 10 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.55 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 0.8 --top_p 0.9 --top_k 100 --system examples/train/grpo/prompt.txt --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs format answer_match_cosine plane_match_string modality_match_string caption_match_cosine --reward_weights 0.05 0.50 0.05 0.05 0.35 --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-5 --lr_scheduler_type cosine --max_steps 10000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 10 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 4 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --temperature 0.8 --top_p 0.9 --top_k 100 --system examples/train/grpo/prompt.txt --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs format answer_match_cosine plane_match_string modality_match_string caption_match_cosine --reward_weights 0.05 0.45 0.05 0.05 0.40 --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 5e-6 --lr_scheduler_type cosine --max_steps 5000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 10 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 4096 --max_length 1024 --max_completion_length 1024 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --system examples/train/grpo/prompt.txt --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs answer_match_cosine title_match_cosine caption_match_cosine plane_match_string modality_match_string format --reward_weights 1.0 0.5 0.5 0.1 0.1 0.5 --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen3-VL-2B-Thinking --report_to wandb
# CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 5e-6 --lr_scheduler_type cosine --max_steps 5000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 10 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 4096 --max_length 1024 --max_completion_length 1024 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --system examples/train/grpo/prompt.txt --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs answer_match_cosine title_match_cosine caption_match_cosine plane_match_string modality_match_string format --reward_weights 1.0 0.5 0.5 0.1 0.1 0.5 --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen3-VL-2B-Instruct --report_to wandb
# CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-4 --lr_scheduler_type cosine --max_steps 5000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 10 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 4096 --max_length 1024 --max_completion_length 1024 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --system examples/train/grpo/prompt.txt --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs answer_match_cosine title_match_cosine caption_match_cosine plane_match_string modality_match_string format --reward_weights 1.5 0.5 0.5 0.1 0.1 0.5 --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen3-VL-2B-Instruct --report_to wandb
# CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-4 --lr_scheduler_type cosine --max_steps 5000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 10 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.55 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --system examples/train/grpo/prompt.txt --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs answer_match_cosine title_match_cosine caption_match_cosine plane_match_string modality_match_string format --reward_weights 1.5 0.5 0.5 0.1 0.1 0.5 --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model LLM-Research/gemma-3-4b-it --report_to wandb
# CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-4 --lr_scheduler_type cosine --max_steps 5000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 10 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 4096 --max_length 1024 --max_completion_length 1024 --num_generations 16 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 16 --temperature 1.0 --top_p 0.9 --top_k 80 --system examples/train/grpo/prompt.txt --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs answer_match_cosine title_match_cosine caption_match_cosine plane_match_string modality_match_string format --reward_weights 1.5 0.5 0.5 0.1 0.1 0.5 --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-4 --lr_scheduler_type cosine --max_steps 10000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 12 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 512 --num_generations 32 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 32 --temperature 1.0 --top_p 0.9 --top_k 80 --system examples/train/grpo/prompt.txt --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs answer_match_cosine title_match_cosine caption_match_cosine plane_match_string modality_match_string format --reward_weights 2.0 0.5 0.5 0.1 0.1 0.1 --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-4 --lr_scheduler_type cosine --max_steps 8000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 10 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.55 --vllm_tensor_parallel_size 1 --vllm_max_model_len 4096 --max_length 1024 --max_completion_length 1024 --num_generations 32 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 32 --temperature 1.0 --top_p 0.9 --top_k 80 --system examples/train/grpo/prompt.txt --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs answer_match_cosine title_match_cosine caption_match_cosine plane_match_string modality_match_string format --reward_weights 2.0 0.5 0.5 0.1 0.1 0.1 --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-4 --lr_scheduler_type cosine --max_steps 8000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 10 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 4096 --max_length 1024 --max_completion_length 1024 --num_generations 16 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 16 --temperature 1.0 --top_p 0.9 --top_k 80 --system examples/train/grpo/prompt.txt --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs answer_match_cosine title_match_cosine caption_match_cosine plane_match_string modality_match_string format --reward_weights 2.0 0.5 0.5 0.1 0.1 0.1 --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-5 --lr_scheduler_type cosine --max_steps 6000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 4096 --max_length 1024 --max_completion_length 512 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 1.0 1.0 0.5 0.1 0.1 0.1 --system /data/workspace/swift/prompt3.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-5 --lr_scheduler_type cosine --max_steps 6000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 4096 --max_length 1024 --max_completion_length 512 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 1.0 1.0 0.5 0.1 0.1 0.5 --system /data/workspace/swift/prompt3.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb --resume_from_checkpoint /data/workspace/swift/output/v99-20251124-233227/checkpoint-1000
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 5e-5 --lr_scheduler_type cosine --max_steps 6000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 512 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 2.0 1.0 0.5 0.1 0.1 0.5 --system /data/workspace/swift/prompt4.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-5 --lr_scheduler_type cosine --max_steps 6000 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 512 --num_generations 32 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 32 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --deepspeed zero0 --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 2.5 1.5 0.2 0.1 0.1 0.1 --system /data/workspace/swift/prompt5.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb --resume_from_checkpoint /data/workspace/swift/output/v108-20251126-225110/checkpoint-500
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 3e-5 --lr_scheduler_type cosine --max_steps 6000 --warmup_ratio 0.05 --logging_steps 1 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 512 --num_generations 16 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 16 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --deepspeed zero2 --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 3.0 1.5 0.2 0.1 0.1 0.1 --system /data/workspace/swift/prompt5.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 3e-5 --lr_scheduler_type cosine --max_steps 6000 --warmup_ratio 0.05 --logging_steps 1 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 32 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 32 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 3.0 1.5 0.2 0.1 0.1 0.1 --system /data/workspace/swift/prompt5.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 3e-5 --lr_scheduler_type cosine --max_steps 6000 --warmup_ratio 0.05 --logging_steps 1 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 32 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 32 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 3.0 1.5 0.2 0.1 0.1 0.1 --system /data/workspace/swift/prompt5.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb --resume_from_checkpoint /data/workspace/swift/output/v113-20251127-180718/checkpoint-4500
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-5 --lr_scheduler_type cosine --max_steps 6000 --warmup_ratio 0.05 --logging_steps 1 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 4 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --temperature 1.0 --top_p 0.95 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine format --reward_weights 1.0 0.1 --system /data/workspace/swift/prompt1.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-5 --lr_scheduler_type cosine --max_steps 6000 --warmup_ratio 0.05 --logging_steps 1 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 4 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --temperature 1.0 --top_p 0.95 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 1.0 1.0 0.2 0.1 0.1 0.1 --system /data/workspace/swift/prompt2.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-4 --lr_scheduler_type cosine --max_steps 6000 --warmup_ratio 0.05 --logging_steps 1 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 4 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --temperature 1.0 --top_p 0.95 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 1.0 1.0 0.2 0.1 0.1 0.1 --system /data/workspace/swift/prompt2.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 2e-5 --lr_scheduler_type constant_with_warmup --max_steps 6000 --warmup_ratio 0.05 --logging_steps 1 --eval_steps 500 --save_steps 500 --save_total_limit 10 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 4 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --temperature 1.0 --top_p 0.95 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 1.5 1.0 0.5 0.1 0.1 0.5 --system /data/workspace/swift/prompt2.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 3e-5 --lr_scheduler_type cosine --max_steps 5000 --warmup_ratio 0.05 --logging_steps 1 --eval_steps 500 --save_steps 500 --save_total_limit 10 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 16 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 16 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 1.5 0.5 0.5 0.1 0.1 0.5 --system /data/workspace/swift/prompt6.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb --resume_from_checkpoint /data/workspace/swift/output/v126-20251203-004753/checkpoint-4500
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 4e-5 --lr_scheduler_type cosine --max_steps 5000 --warmup_ratio 0.05 --logging_steps 1 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 1.5 0.5 0.5 0.1 0.1 0.5 --system /data/workspace/swift/prompt6.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 4e-5 --lr_scheduler_type cosine --max_steps 5000 --warmup_ratio 0.05 --logging_steps 1 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format reasoning_consistency_nli_mini --reward_weights 1.5 0.5 0.5 0.1 0.1 0.5 1.0 --system /data/workspace/swift/prompt6.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 2e-5 --lr_scheduler_type cosine --max_steps 5000 --warmup_ratio 0.05 --logging_steps 10 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format reasoning_consistency_nli_mini --reward_weights 1.5 0.5 0.5 0.1 0.1 0.5 0.5 --system /data/workspace/swift/prompt6.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 3e-5 --lr_scheduler_type cosine --max_steps 5000 --warmup_ratio 0.05 --logging_steps 10 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attention_2 --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 1.5 0.5 0.5 0.1 0.1 0.5 --system /data/workspace/swift/prompt7.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift sft --train_type lora --lora_rank 8 --lora_alpha 32 --target_modules all-linear --max_steps 10000 --save_total_limit 5 --save_steps 100 --learning_rate 1.0e-4 --lr_scheduler_type constant --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --max_length 4096 --dataloader_num_workers 1 --disable_tqdm True --tuner_backend peft --attn_impl flash_attn --freeze_vit False --freeze_aligner False --logging_steps 10 --eval_steps 20000 --save_only_model True --model OpenBMB/MiniCPM-V --dataset /home/civisky/workspace/datasets/pathvqa/subset/minicpm-v/pathvqa_train_init0.05_ceal0.05-55-x1.jsonl --val_dataset /home/civisky/workspace/datasets/pathvqa/pathvqa_test.jsonl
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 3e-5 --lr_scheduler_type cosine --max_steps 5000 --warmup_ratio 0.05 --logging_steps 10 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attention_2 --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 1.5 0.5 0.5 0.1 0.1 0.5 --system /data/workspace/swift/prompt7.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb --answer_match_cosine_model_name pritamdeka/S-BioBERT-snli-multinli-stsb --answer_match_cosine_threshold 0.50 --caption_match_cosine_model_name pritamdeka/S-BioBERT-snli-multinli-stsb --caption_match_cosine_threshold 0.30 --title_match_cosine_model_name pritamdeka/S-BioBERT-snli-multinli-stsb --title_match_cosine_threshold 0.30
# PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 1e-5 --lr_scheduler_type cosine --max_steps 5000 --warmup_ratio 0.05 --logging_steps 10 --eval_steps 500 --save_steps 500 --save_total_limit 5 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attention_2 --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 1.5 0.5 0.5 0.1 0.1 0.5 --system /data/workspace/swift/prompt7.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb --answer_match_cosine_model_name pritamdeka/S-BioBERT-snli-multinli-stsb --answer_match_cosine_threshold 0.50 --caption_match_cosine_model_name pritamdeka/S-BioBERT-snli-multinli-stsb --caption_match_cosine_threshold 0.30 --title_match_cosine_model_name pritamdeka/S-BioBERT-snli-multinli-stsb --title_match_cosine_threshold 0.30 --resume_from_checkpoint /data/workspace/swift/output/v166-20251209-103641/checkpoint-2000
PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' CUDA_VISIBLE_DEVICES=0 NPROC_PER_NODE=1 MAX_PIXELS=602112 swift rlhf --rlhf_type grpo --train_type lora --load_from_cache_file true --torch_dtype bfloat16 --learning_rate 2e-5 --lr_scheduler_type cosine --max_steps 5500 --warmup_ratio 0.05 --eval_steps 500 --save_steps 500 --save_total_limit 5 --logging_steps 10 --output_dir output --dataloader_num_workers 1 --attn_impl flash_attn --use_vllm true --vllm_mode colocate --vllm_gpu_memory_utilization 0.50 --vllm_tensor_parallel_size 1 --vllm_max_model_len 2048 --max_length 1024 --max_completion_length 1024 --num_generations 8 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --temperature 1.0 --top_p 0.9 --top_k 80 --log_completions true --offload_optimizer true --offload_model true --reward_funcs answer_match_cosine caption_match_cosine title_match_cosine plane_match_string modality_match_string format --reward_weights 1.5 0.5 0.5 0.1 0.1 0.5 --system /data/workspace/swift/prompt7.txt --dataset /data/datasets/vqarad/vqarad_train_rl.jsonl --model Qwen/Qwen2.5-VL-3B-Instruct --report_to wandb --answer_match_cosine_model_name pritamdeka/S-BioBERT-snli-multinli-stsb --answer_match_cosine_threshold 0.50 --caption_match_cosine_model_name pritamdeka/S-BioBERT-snli-multinli-stsb --caption_match_cosine_threshold 0.30 --title_match_cosine_model_name pritamdeka/S-BioBERT-snli-multinli-stsb --title_match_cosine_threshold 0.30 --enable_golden_truth_injection false


























































